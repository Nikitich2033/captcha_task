{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1110 files belonging to 9 classes.\n",
      "Found 366 files belonging to 9 classes.\n",
      "Epoch 1/3\n",
      "35/35 [==============================] - 2s 23ms/step - loss: 1.2929 - accuracy: 0.6234\n",
      "Epoch 2/3\n",
      "35/35 [==============================] - 1s 23ms/step - loss: 0.3186 - accuracy: 0.9180\n",
      "Epoch 3/3\n",
      "35/35 [==============================] - 1s 23ms/step - loss: 0.1829 - accuracy: 0.9541\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3711 - accuracy: 0.9044\n",
      "Test accuracy: 0.9043715596199036\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and preprocess your data\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'sorted_training/train',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=['1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
    "    image_size=(48,30),\n",
    "    batch_size=32)\n",
    "\n",
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'sorted_training/test',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=['1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
    "    image_size=(48,30),\n",
    "    batch_size=128)\n",
    "\n",
    "# Build your model\n",
    "model = keras.Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(9, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile your model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Train your model\n",
    "model.fit(train_ds, epochs=3)\n",
    "\n",
    "# Evaluate your model\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "# Save the model\n",
    "model.save(\"custom_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized digits: 333333\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_model = load_model(\"custom_model.h5\")\n",
    "\n",
    "def segment_img(input_img):\n",
    "    # Convert the PIL Image to a NumPy array\n",
    "    image_array = np.array(input_img)\n",
    "    \n",
    "    # Image processing\n",
    "    # Convert to grayscale\n",
    "    img = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    th = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 17, 2)\n",
    "\n",
    "    # Otsu thresholding\n",
    "    ret2, th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Otsu thresholding with Gaussian blur\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "\n",
    "    dilation = cv2.dilate(th3, kernel, iterations=1)\n",
    "\n",
    "    erosion = cv2.erode(dilation, kernel, iterations=1)\n",
    "\n",
    "    kernel = np.ones((3,1), np.uint8)\n",
    "    dilation = cv2.dilate(erosion, kernel, iterations=2)\n",
    "\n",
    "    # Get individual letters\n",
    "    x, y, w, h = 22, 10, 20, 38\n",
    "    segments = []\n",
    "    for i in range(6):\n",
    "        # Save each character as a separate image\n",
    "        digit = dilation[y:y + h, x:x + w]\n",
    "\n",
    "        # Add a white border of 8 pixels\n",
    "        digit = cv2.copyMakeBorder(digit, 5, 5, 5, 5, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "\n",
    "        # Convert the digit to PIL Image\n",
    "        digit_pil = Image.fromarray(digit)\n",
    "\n",
    "        # Append the digit to the list of segments\n",
    "        segments.append(digit_pil)\n",
    "\n",
    "        x += w\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Load the image file\n",
    "image_file = \"captcha_imgs/captcha_21.jpg\"\n",
    "with open(image_file, 'rb') as file:\n",
    "    image_data = base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "# Decode image from base64 and create an Image object\n",
    "image_bytes = base64.b64decode(image_data)\n",
    "image = Image.open(BytesIO(image_bytes))\n",
    "\n",
    "# Segment the image\n",
    "segmented_images = segment_img(image)\n",
    "\n",
    "# Preprocess and display all segments using Matplotlib\n",
    "preprocessed_segments = []\n",
    "for i, segment in enumerate(segmented_images):\n",
    "    # Preprocess the segment\n",
    "    segment = np.array(segment)\n",
    "    # segment = cv2.resize(segment, (28, 28))\n",
    "    segment = np.invert(segment)\n",
    "    segment = segment.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "    segment = np.expand_dims(segment, axis=-1)  # Add channel dimension\n",
    "    \n",
    "    preprocessed_segments.append(segment)\n",
    "    \n",
    "# Predict the digits and print the images\n",
    "recognized_text = \"\"\n",
    "fig, axs = plt.subplots(1, len(preprocessed_segments), figsize=(10, 4))\n",
    "for i, segment in enumerate(preprocessed_segments):\n",
    "    # Reshape and resize the segment for prediction\n",
    "    segment = segment.reshape(48, 30, 1)\n",
    "    segment = tf.image.resize(segment, (48, 30))\n",
    "    segment = np.concatenate([segment, segment, segment], axis=-1)\n",
    "    \n",
    "    # Predict the digit using the pre-trained Keras MNIST model\n",
    "    digit = mnist_model.predict(np.array([segment]))\n",
    "    \n",
    "    # Append the recognized digit to the recognized text\n",
    "    recognized_digit = str(np.argmax(digit))\n",
    "    recognized_text += recognized_digit\n",
    "    \n",
    "    # Display the segment image with the recognized digit\n",
    "    axs[i].imshow(segment, plt.cm.binary)\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(f\"Digit {i+1}: {recognized_digit}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Recognized digits:\", recognized_text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
